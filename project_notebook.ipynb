{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPvFmg1I97sRMgEE6Q+7wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeoanni/Stats-507-final-project/blob/main/project_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Anomaly Detection Project"
      ],
      "metadata": {
        "id": "LsRHkVND0soX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the Time Series Anomaly Detection project! In this notebook, we will walk through the implementation step-by-step, explaining each part of the code and how it contributes to the overall goal of detecting anomalies in time series data."
      ],
      "metadata": {
        "id": "T4vwA_r30dkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "0UkhSEPt0gtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's make sure we have the necessary libraries installed. We'll be using NumPy, Pandas, Scikit-learn, and PyTorch."
      ],
      "metadata": {
        "id": "50ynATLG0iOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxRDgbPb0ltt",
        "outputId": "aa5ee68e-75c6-4e31-8005-1a725cbcb7fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocess.py"
      ],
      "metadata": {
        "id": "pI4Q8h2QwcdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains the data preprocessing code for the time series anomaly detection project. It defines two main classes: TimeSeriesDataset and TimeSeriesPreprocessor. The TimeSeriesDataset class is a custom PyTorch dataset that handles the loading and formatting of time series sequences and their corresponding labels. The TimeSeriesPreprocessor class provides methods for adding enhanced time features to the raw data, creating sequences with a specified window size and stride, and generating PyTorch DataLoader objects for training and validation."
      ],
      "metadata": {
        "id": "zOAM57Riy5F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To use this preprocessing code, create an instance of the TimeSeriesPreprocessor class and call the create_dataloaders method. This will load the default dataset, preprocess it, and return PyTorch DataLoader objects for training and validation. You can also pass a custom DataFrame to the create_dataloaders method if you want to use your own data."
      ],
      "metadata": {
        "id": "L3UKhkeby6Jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "class TimeSeriesPreprocessor:\n",
        "    def __init__(self, window_size=288, stride=12):\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def add_time_features(self, df):\n",
        "        df = df.copy()\n",
        "\n",
        "        df['hour'] = df.timestamp.dt.hour\n",
        "        df['day_of_week'] = df.timestamp.dt.dayofweek\n",
        "        df['day_of_month'] = df.timestamp.dt.day\n",
        "\n",
        "        df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
        "        df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
        "        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
        "        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
        "\n",
        "        df['rolling_mean'] = df['value'].rolling(window=12, min_periods=1).mean()\n",
        "        df['rolling_std'] = df['value'].rolling(window=12, min_periods=1).std()\n",
        "        df['rolling_max'] = df['value'].rolling(window=12, min_periods=1).max()\n",
        "        df['rolling_min'] = df['value'].rolling(window=12, min_periods=1).min()\n",
        "\n",
        "        df['lag_1'] = df['value'].shift(1)\n",
        "        df['lag_6'] = df['value'].shift(6)\n",
        "        df['lag_12'] = df['value'].shift(12)\n",
        "\n",
        "        df = df.fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_sequences(self, df):\n",
        "        feature_columns = ['value', 'rolling_mean', 'rolling_std', 'rolling_max',\n",
        "                         'rolling_min', 'lag_1', 'lag_6', 'lag_12',\n",
        "                         'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
        "\n",
        "        scaled_features = self.scaler.fit_transform(df[feature_columns])\n",
        "\n",
        "        sequences = []\n",
        "        labels = []\n",
        "\n",
        "        mean = df['value'].mean()\n",
        "        std = df['value'].std()\n",
        "        upper_threshold = mean + 2*std\n",
        "        lower_threshold = mean - 2*std\n",
        "\n",
        "        for i in range(0, len(df) - self.window_size, self.stride):\n",
        "            seq = scaled_features[i:i + self.window_size]\n",
        "            next_val = df['value'].iloc[i + self.window_size]\n",
        "\n",
        "            is_anomaly = (next_val > upper_threshold) or (next_val < lower_threshold)\n",
        "\n",
        "            sequences.append(seq)\n",
        "            labels.append(float(is_anomaly))\n",
        "\n",
        "            if is_anomaly:\n",
        "                noise = np.random.normal(0, 0.1, seq.shape)\n",
        "                augmented_seq = seq + noise\n",
        "                sequences.append(augmented_seq)\n",
        "                labels.append(1.0)\n",
        "\n",
        "        return np.array(sequences), np.array(labels)\n",
        "\n",
        "    def create_dataloaders(self, df=None, batch_size=16, train_split=0.8):\n",
        "        if df is None:\n",
        "            base_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
        "            file_path = f\"{base_url}realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv\"\n",
        "            df = pd.read_csv(file_path)\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "        df = self.add_time_features(df)\n",
        "        sequences, labels = self.create_sequences(df)\n",
        "\n",
        "        indices = np.random.permutation(len(sequences))\n",
        "        sequences = sequences[indices]\n",
        "        labels = labels[indices]\n",
        "\n",
        "        train_size = int(len(sequences) * train_split)\n",
        "\n",
        "        train_sequences = sequences[:train_size]\n",
        "        train_labels = labels[:train_size]\n",
        "        val_sequences = sequences[train_size:]\n",
        "        val_labels = labels[train_size:]\n",
        "\n",
        "        train_dataset = TimeSeriesDataset(train_sequences, train_labels)\n",
        "        val_dataset = TimeSeriesDataset(val_sequences, val_labels)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "        return train_loader, val_loader"
      ],
      "metadata": {
        "id": "DbLD3DehwkFl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer.py"
      ],
      "metadata": {
        "id": "sCSXdze7w43m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains the implementation of the Transformer-based anomaly detection model. It defines two main classes: PositionalEncoding and TransformerAnomaly. The PositionalEncoding class is used to add positional information to the input sequences, which is important for the Transformer architecture. The TransformerAnomaly class defines the architecture of the anomaly detection model, including the input projection, positional encoding, Transformer encoder, and output layers."
      ],
      "metadata": {
        "id": "-KprGYVFzKZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To use the TransformerAnomaly model, create an instance of the class with the desired hyperparameters (e.g., input_dim, d_model, nhead, num_layers, dropout) and call the forward method with your input sequences. The model will return the anomaly probabilities for each input sequence."
      ],
      "metadata": {
        "id": "tP6_CBQrzL8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=288):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class TransformerAnomaly(nn.Module):\n",
        "    def __init__(self, input_dim=12, d_model=32, nhead=4, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_model*4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(d_model * 288, 64)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.input_projection(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.flatten(output)\n",
        "        output = torch.relu(self.fc1(output))\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc2(output)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "-WZe7ULbw9sy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "TDNI5OVWxF1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains the training code for the Transformer-based anomaly detection model. It loads and preprocesses the data using the TimeSeriesPreprocessor, initializes the TransformerAnomaly model, and defines the loss function (WeightedBCELoss) and optimizer (Adam). The code then trains the model for a specified number of epochs, using early stopping with a minimum improvement threshold to prevent overfitting. The training history is plotted and saved at the end."
      ],
      "metadata": {
        "id": "jngHw43-zSEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To train the model, simply run this script. It will load the data, initialize the model, and start the training process. The best model will be saved in the models/saved directory, and the training history plot will be saved in the results directory."
      ],
      "metadata": {
        "id": "nufyYc0uzTX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from utils.preprocess import TimeSeriesPreprocessor\n",
        "from models.transformer import TransformerAnomaly\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "class WeightedBCELoss(nn.Module):\n",
        "    def __init__(self, pos_weight):\n",
        "        super().__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        weight = torch.where(target == 1, self.pos_weight, torch.tensor(1.0))\n",
        "        loss = -(weight * (target * torch.log(pred + 1e-10) + \\\n",
        "                (1 - target) * torch.log(1 - pred + 1e-10)))\n",
        "        return loss.mean()\n",
        "\n",
        "def main():\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logger.info(f\"Using device: {device}\")\n",
        "\n",
        "    logger.info(\"Loading and preprocessing data...\")\n",
        "    preprocessor = TimeSeriesPreprocessor()\n",
        "    train_loader, val_loader = preprocessor.create_dataloaders(batch_size=16)\n",
        "\n",
        "    logger.info(\"Initializing model...\")\n",
        "    model = TransformerAnomaly(input_dim=12, d_model=64, nhead=4, num_layers=3, dropout=0.2).to(device)\n",
        "\n",
        "    pos_weight = torch.tensor((1 - 0.0327) / 0.0327)\n",
        "    criterion = WeightedBCELoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "    num_epochs = 100\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    logger.info(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch, labels in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch, labels in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(batch)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        logger.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        logger.info(f\"Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            logger.info(\"Saving new best model...\")\n",
        "            torch.save(model.state_dict(), Path('models/saved/best_model_realAWS.pth'))\n",
        "\n",
        "    Path('results').mkdir(exist_ok=True, parents=True)\n",
        "    np.save('results/train_losses_realAWS.npy', train_losses)\n",
        "    np.save('results/val_losses_realAWS.npy', val_losses)\n",
        "    logger.info(\"Training complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Vegi1c20xINn",
        "outputId": "1813f950-cb10-47e8-9fd5-b0c7850243b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-89daaa933845>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeSeriesPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerAnomaly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate.py"
      ],
      "metadata": {
        "id": "EqtZ4Iu5xRN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains code for evaluating the trained anomaly detection model using precision-recall and ROC curves. It loads the best model from the models/saved directory, makes predictions on the entire dataset, and calculates the precision, recall, and false positive rates at different threshold values. It then plots the precision-recall and ROC curves, calculates the area under each curve (PR-AUC and ROC-AUC), and saves the plots and metrics in the results directory."
      ],
      "metadata": {
        "id": "dZSlXJiOzbpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To evaluate the model, run this script after training the model. The performance curves and metrics will be saved in the results directory."
      ],
      "metadata": {
        "id": "jtdckUOrzdCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "from utils.preprocess import TimeSeriesPreprocessor\n",
        "from models.transformer import TransformerAnomaly\n",
        "from pathlib import Path\n",
        "\n",
        "def evaluate_model(model_path='models/saved/best_model.pth'):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    preprocessor = TimeSeriesPreprocessor()\n",
        "    base_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
        "    file_path = f\"{base_url}realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    print(\"Preparing evaluation data...\")\n",
        "    df_eval = preprocessor.add_time_features(df)\n",
        "    sequences, labels = preprocessor.create_sequences(df_eval)\n",
        "\n",
        "    print(\"Loading model...\")\n",
        "    model = TransformerAnomaly().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "    predictions = []\n",
        "    batch_size = 32\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(sequences), batch_size):\n",
        "            batch = sequences[i:i + batch_size]\n",
        "            batch_tensor = torch.FloatTensor(batch).to(device)\n",
        "            outputs = model(batch_tensor)\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions).flatten()\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
        "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(\"Generating visualizations...\")\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # PR Curve\n",
        "    ax1.plot(recall, precision)\n",
        "    ax1.set_title(f'Precision-Recall Curve (AUC = {pr_auc:.3f})')\n",
        "    ax1.set_xlabel('Recall')\n",
        "    ax1.set_ylabel('Precision')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # ROC Curve\n",
        "    ax2.plot(fpr, tpr)\n",
        "    ax2.set_title(f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    Path('results').mkdir(exist_ok=True)\n",
        "\n",
        "    plt.savefig('results/performance_curves.png')\n",
        "    print(\"Results saved to results/performance_curves.png\")\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "    print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_model()"
      ],
      "metadata": {
        "id": "zJh5yt2FxWlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analyze_results.py"
      ],
      "metadata": {
        "id": "Kr8TeciFxgmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains code for analyzing the performance of the trained anomaly detection model. It loads the best model from the models/saved directory and makes predictions on the entire dataset. It then generates a comprehensive analysis report, including a time series plot with detected anomalies, a plot of anomaly scores over time, and a histogram of the anomaly score distribution. The report also includes key metrics such as the total number of windows analyzed, the number and percentage of detected anomalies, and summary statistics of the anomaly scores."
      ],
      "metadata": {
        "id": "fR9XUTdezjZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To perform the analysis, run this script after training the model. The generated report will be saved in the results/analysis_report directory."
      ],
      "metadata": {
        "id": "AM5f01RZzoAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from utils.preprocess import TimeSeriesPreprocessor\n",
        "from models.transformer import TransformerAnomaly\n",
        "from pathlib import Path\n",
        "\n",
        "class ModelAnalyzer:\n",
        "    def __init__(self, model_path='models/saved/best_model.pth'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.preprocessor = TimeSeriesPreprocessor()\n",
        "        self.model = TransformerAnomaly().to(self.device)\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        else:\n",
        "            self.model.load_state_dict(checkpoint)\n",
        "        self.model.eval()\n",
        "\n",
        "    def analyze_performance(self):\n",
        "        print(\"Loading data...\")\n",
        "        base_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
        "        file_path = f\"{base_url}realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "        print(\"Making predictions...\")\n",
        "        df_eval = self.preprocessor.add_time_features(df)\n",
        "        sequences, labels = self.preprocessor.create_sequences(df_eval)\n",
        "\n",
        "        predictions = []\n",
        "        batch_size = 32\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(sequences), batch_size):\n",
        "                batch = sequences[i:i + batch_size]\n",
        "                batch_tensor = torch.FloatTensor(batch).to(self.device)\n",
        "                outputs = self.model(batch_tensor)\n",
        "                predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "        predictions = np.array(predictions).flatten()\n",
        "\n",
        "        print(\"Generating visualizations...\")\n",
        "        self.generate_report(predictions, labels, df)\n",
        "\n",
        "    def generate_report(self, predictions, labels, df):\n",
        "        save_path = 'results/analysis_report'\n",
        "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 1. Time Series Plot\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        plt.subplot(3, 1, 1)\n",
        "        window_size = self.preprocessor.window_size\n",
        "        valid_timestamps = df['timestamp'][window_size:len(predictions)+window_size]\n",
        "        plt.plot(df['timestamp'], df['value'], label='Original', alpha=0.7)\n",
        "\n",
        "        threshold = 0.3\n",
        "        anomaly_mask = predictions > threshold\n",
        "        anomaly_times = valid_timestamps[anomaly_mask]\n",
        "        anomaly_values = df['value'][window_size:len(predictions)+window_size][anomaly_mask]\n",
        "        plt.scatter(anomaly_times, anomaly_values, color='red', label=f'Detected Anomalies (threshold={threshold})')\n",
        "        plt.title('CPU Utilization with Detected Anomalies')\n",
        "        plt.legend()\n",
        "\n",
        "        # 2. Prediction Scores\n",
        "        plt.subplot(3, 1, 2)\n",
        "        plt.plot(valid_timestamps, predictions, label='Anomaly Score')\n",
        "        plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\n",
        "        plt.title('Anomaly Scores Over Time')\n",
        "        plt.legend()\n",
        "\n",
        "        # 3. Score Distribution\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.hist(predictions, bins=50, density=True, alpha=0.7)\n",
        "        plt.axvline(x=threshold, color='r', linestyle='--', label='Threshold')\n",
        "        plt.title('Distribution of Anomaly Scores')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{save_path}/anomaly_detection_results.png')\n",
        "        plt.close()\n",
        "\n",
        "        with open(f'{save_path}/detection_report.txt', 'w') as f:\n",
        "            f.write(\"Anomaly Detection Report\\n\")\n",
        "            f.write(\"======================\\n\\n\")\n",
        "            f.write(f\"Total windows analyzed: {len(predictions)}\\n\")\n",
        "            f.write(f\"Number of anomalies detected: {np.sum(anomaly_mask)}\\n\")\n",
        "            f.write(f\"Percentage of anomalies: {100 * np.sum(anomaly_mask) / len(predictions):.2f}%\\n\")\n",
        "            f.write(f\"\\nThreshold used: {threshold}\\n\")\n",
        "\n",
        "            mean_score = np.mean(predictions)\n",
        "            std_score = np.std(predictions)\n",
        "            f.write(f\"\\nScore Statistics:\\n\")\n",
        "            f.write(f\"Mean score: {mean_score:.4f}\\n\")\n",
        "            f.write(f\"Standard deviation: {std_score:.4f}\\n\")\n",
        "            f.write(f\"Min score: {np.min(predictions):.4f}\\n\")\n",
        "            f.write(f\"Max score: {np.max(predictions):.4f}\\n\")\n",
        "\n",
        "def main():\n",
        "    analyzer = ModelAnalyzer()\n",
        "    analyzer.analyze_performance()\n",
        "    print(\"Analysis complete! Check results/analysis_report/ for detailed findings.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XfaT-PzMxpPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# baseline_comparison.py"
      ],
      "metadata": {
        "id": "TqNDK4vKx1dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains code for comparing the Transformer-based anomaly detection model with two baseline models: an LSTM-based model and a simple moving average model. It includes the implementation of the LSTMAnomalyDetector class, which defines an LSTM-based anomaly detection model. The script generates synthetic data, trains the LSTM model, and evaluates both the LSTM and moving average models using PR-AUC and ROC-AUC metrics."
      ],
      "metadata": {
        "id": "4bMgha8s0L2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To use this code, you can run the script as is to compare the performance of the LSTM and moving average models on synthetic data. The script will train the LSTM model for a few epochs and then evaluate both models using the precision_recall_curve and roc_curve functions from scikit-learn. The PR-AUC and ROC-AUC scores will be printed for each model, allowing you to compare their performance. You can modify the synthetic data generation or add your own data loading functions to test the models on different datasets."
      ],
      "metadata": {
        "id": "aHHX_EGP0Mva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LSTMAnomalyDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMAnomalyDetector, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        if len(out.shape) == 3:\n",
        "            out = out[:, -1, :]\n",
        "        return self.fc(out)\n",
        "\n",
        "\n",
        "def create_synthetic_data(num_samples=1000, num_features=12, anomaly_rate=0.05):\n",
        "    data = np.random.rand(num_samples, num_features)\n",
        "    labels = (np.random.rand(num_samples) < anomaly_rate).astype(int)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def create_dataloaders(data, labels, batch_size=32):\n",
        "    data = torch.tensor(data, dtype=torch.float32)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    dataset = TimeSeriesDataset(data, labels)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs).squeeze()\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_scores.extend(outputs.cpu().numpy())\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return {'PR-AUC': pr_auc, 'ROC-AUC': roc_auc}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    data, labels = create_synthetic_data()\n",
        "    dataloader = create_dataloaders(data, labels, batch_size=16)\n",
        "\n",
        "    lstm_model = LSTMAnomalyDetector(input_dim=12, hidden_dim=64, num_layers=2, output_dim=1).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "    lstm_model.train()\n",
        "    for epoch in range(10):\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = lstm_model(inputs).squeeze()\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/10, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    lstm_results = evaluate_model(lstm_model, dataloader, device)\n",
        "    print(f\"LSTM Results: PR-AUC = {lstm_results['PR-AUC']:.3f}, ROC-AUC = {lstm_results['ROC-AUC']:.3f}\")\n",
        "\n",
        "    data_flat = data[:, -1]\n",
        "    moving_avg = np.convolve(data_flat, np.ones(5)/5, mode='same')\n",
        "    anomaly_scores = np.abs(data_flat - moving_avg)\n",
        "    precision, recall, _ = precision_recall_curve(labels, anomaly_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    fpr, tpr, _ = roc_curve(labels, anomaly_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f\"Moving Average Results: PR-AUC = {pr_auc:.3f}, ROC-AUC = {roc_auc:.3f}\")\n"
      ],
      "metadata": {
        "id": "2GRW6Ulgx6F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualize_results.py"
      ],
      "metadata": {
        "id": "2j26k4joyIcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**: This file contains code for visualizing and comparing the performance of different anomaly detection models using bar plots. It plots the Precision-Recall AUC (PR-AUC) and ROC-AUC scores for three models: Transformer, LSTM, and Moving Average. The bar plots provide a clear visual comparison of the models' performance on these metrics."
      ],
      "metadata": {
        "id": "uhO9O13N0FUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Guidance**: To use this code, make sure you have the necessary performance metrics (PR-AUC and ROC-AUC scores) for each model you want to compare. Update the models list with the names of your models, and the pr_auc_scores and roc_auc_scores lists with their corresponding scores. Run the script to generate the bar plots, which will help you quickly assess and compare the performance of different anomaly detection models."
      ],
      "metadata": {
        "id": "7IPaRk3H0G9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\"Transformer\", \"LSTM\", \"Moving Average\"]\n",
        "pr_auc_scores = [0.857, 0.057, 0.045]\n",
        "roc_auc_scores = [0.976, 0.545, 0.497]\n",
        "\n",
        "# Bar Plot for PR-AUC\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(models, pr_auc_scores, color=\"blue\", alpha=0.7)\n",
        "plt.title(\"Precision-Recall AUC Comparison\")\n",
        "plt.ylabel(\"PR-AUC\")\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# Bar Plot for ROC-AUC\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(models, roc_auc_scores, color=\"green\", alpha=0.7)\n",
        "plt.title(\"ROC-AUC Comparison\")\n",
        "plt.ylabel(\"ROC-AUC\")\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OnyqIdpuySzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "T2xE25jq09ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code files implement a comprehensive pipeline for anomaly detection in time series data using a Transformer-based approach. The pipeline includes data preprocessing, model architecture definition, training, evaluation, results analysis, visualization, and comparison with baseline models. The main focus is on the Transformer-based anomaly detection model, which leverages the power of self-attention mechanisms to capture complex patterns and dependencies in time series data.\n",
        "The pipeline is designed to be modular and extensible, allowing users to easily adapt it to different datasets and model architectures. The code is well-organized and follows best practices for data preprocessing, model training, and evaluation. The use of PyTorch and popular libraries like scikit-learn ensures compatibility and ease of use.\n",
        "The included baseline comparison code provides a way to evaluate the performance of the Transformer-based model against traditional approaches like LSTM and moving average. This enables users to assess the effectiveness of the Transformer architecture in detecting anomalies and make informed decisions about model selection."
      ],
      "metadata": {
        "id": "kwMTpJDS0_VU"
      }
    }
  ]
}